---
---

@article{mercea2024time,
  title={Time-Memory-and Parameter-Efficient Visual Adaptation},
  author={Mercea, Otniel-Bogdan and Gritsenko, Alexey and Schmid, Cordelia and Arnab, Anurag},
  note={SPOTLIGHT @ CVPR},
  selected={true},
  year={2024},
  preview={losa.png},
  google_scholar_id={eQOLeE2rZwMC},
  type={paper},
  pdf={https://arxiv.org/pdf/2402.02887},
  video={https://www.youtube.com/watch?v=fUIk8LSpByw},
  location={Seattle, USA},
  cited_by={Harvard, University of Cambridge, Google DeepMind, Stanford, Chinese Academy of Sciences},
}

@article{kurzendorfer2024audio,
  title={Audio-Visual Generalized Zero-Shot Learning using Pre-Trained Large Multi-Modal Models},
  author={Kurzend{\"o}rfer*, David and Mercea*, Otniel-Bogdan and Koepke, A and Akata, Zeynep},
  note={CVPRW},
  selected={false},
  year={2024},
  preview={clip_clap.png},
  type={paper},
  google_scholar_id={WF5omc3nYNoC},
  pdf={https://arxiv.org/pdf/2404.06309},
  code={https://github.com/dkurzend/ClipClap-GZSL},
  location={Seattle, USA},
  github_repo={dkurzend/ClipClap-GZSL},
  cited_by={Tsinghua University}
}

@article{nowak2024towards,
  title={Towards Optimal Adapter Placement for Efficient Transfer Learning},
  author={Nowak, Aleksandra I and Mercea, Otniel-Bogdan and Arnab, Anurag and Pfeiffer, Jonas and Dauphin, Yann and Evci, Utku},
  note={arXiv preprint arXiv:2410.15858},
  year={2024},
  selected={false},
  preview={towards.png},
  type={paper},
  google_scholar_id={LkGwnXOMwfcC},
  pdf={https://arxiv.org/pdf/2410.15858},
}

@article{Hummel_2023_BMVC,
author    = {Thomas Hummel and Otniel-Bogdan Mercea and A. Sophia Koepke and Zeynep Akata},
title     = {Video-adverb retrieval with compositional adverb-action embeddings},
note   = {ORAL @ BMVC},
publisher = {BMVA},
year      = {2023},
preview   = {regada.png},
type      = {paper},
selected  = {false},
google_scholar_id={YsMSGLbcyi4C},
pdf={https://arxiv.org/pdf/2309.15086},
video={https://bmvc2022.mpi-inf.mpg.de/BMVC2023/0581_video.mp4},
code={https://github.com/ExplainableML/ReGaDa},
github_repo={ExplainableML/ReGaDa},
location={Aberdeen, UK},
cited_by={University of Edinburgh},
}


@article{mercea2023text,
  title={Text-to-feature diffusion for audio-visual few-shot learning},
  author={Mercea, Otniel-Bogdan and Hummel, Thomas and Koepke, A Sophia and Akata, Zeynep},
  note={DAGM GCPR},
  selected={true},
  year={2023},
  preview={avdiff.jpeg},
  type={paper},
  google_scholar_id={W7OEmFMy1HYC},
  pdf={https://arxiv.org/pdf/2309.03869},
  code={https://github.com/ExplainableML/AVDIFF-GFSL},
  github_repo={ExplainableML/AVDIFF-GFSL},
  location={Heidelberg, Germany},
  cited_by={Technical University of Munich, Chinese Academy of Sciences},
}

@article{renz2022plant,
  title={PlanT: Explainable Planning Transformers via Object-Level Representations},
  author={Renz, Katrin and Chitta, Kashyap and Mercea, Otniel-Bogdan and Koepke, A and Akata, Zeynep and Geiger, Andreas},
  note={CoRL},
  year={2022},
  preview={plant.png},
  type={paper},
  selected={false},
  google_scholar_id={9yKSN-GCB0IC},
  pdf={https://arxiv.org/pdf/2210.14222},
  video={https://www.youtube.com/watch?v=_sNNEyjMmaY},
  code={https://github.com/autonomousvision/plant},
  github_repo={autonomousvision/plant},
  location={Auckland, New Zealand},
  cited_by={University of Oxford, Harvard, Google, Stanford, Nvidia},
}

@article{mercea2022temporal,
  title={Temporal and cross-modal attention for audio-visual zero-shot learning},
  author={Mercea*, Otniel-Bogdan and Hummel*, Thomas and Koepke, A Sophia and Akata, Zeynep},
  note={ECCV},
  selected={true},
  year={2022},
  type={paper},
  preview={tcaf.png},
  google_scholar_id={d1gkVwhDpl0C},
  pdf={https://arxiv.org/pdf/2207.09966},
  code={https://github.com/ExplainableML/TCAF-GZSL},
  github_repo={ExplainableML/TCAF-GZSL},
  location={Tel Aviv, Israel},
  cited_by={Carnegie Mellon University, Amazon, Australian National University, Technical University of Munich, Tsinghua University},
}

@article{mercea2022audio,
  title={Audio-visual generalised zero-shot learning with cross-modal attention and language},
  author={Mercea, Otniel-Bogdan and Riesch, Lukas and Koepke, A and Akata, Zeynep},
  note={CVPR},
  selected={true},
  year={2022},
  preview={avca.png},
  type={paper}, 
  google_scholar_id={u-x6o8ySG0sC},
  pdf={https://arxiv.org/pdf/2203.03598},
  code={https://github.com/ExplainableML/AVCA-GZSL},
  github_repo={ExplainableML/AVCA-GZSL},
  location={New Orleans, USA},
  cited_by={Google Research, Carnegie Mellon University, Amazon, Mila, Cornell University},
}

@misc{todoran2023system,
  title={System and method for tracking and identifying moving objects},
  author={Todoran, Ana Cristina and Mercea, Otniel-Bogdan},
  year={2023},
  publisher={Google Patents},
  note={US Patent App. 17/562,364},
  type={patent},
  preview={patent_tracking.png},
  google_scholar_id={zYLM7Y9cAGgC},
  selected={true},
  pdf={https://patentimages.storage.googleapis.com/87/fa/5a/b3d6d9f2a087c3/US20230206466A1.pdf}
}

@misc{todoran2023system,
  title={System and method for adjusting a position of an order taking device},
  author={Todoran, Ana Cristina and Mercea, Otniel-Bogdan and Cioarga, Razvan-Dorel},
  year={2023},
  publisher={Google Patents},
  note={US Patent App. 17/562,365},
  type={patent},
  preview={patent_adjusting.png},
  google_scholar_id={Tyk-4Ss8FVUC},
  selected={true},
  pdf={https://patentimages.storage.googleapis.com/37/6b/1b/f37bdae2c41124/US20230200569A1.pdf}
}
